---
output:
  pdf_document: default
  html_document: default
---
There are 2 packages you will need to install for today's practical: `install.packages(c("h2o", "eegkit", "forecast", "tseries")` apart from that everything else should already be available on your system.
However, I will endeavour to use explicit imports to make it clear where functions are coming from (functions without `library_name::` are part of base R or a function we've defined in this notebook).

```{r setup}
knitr::opts_chunk$set(echo = TRUE)

# experimenting with this ML library on my quest to find something pleasant to use in R
library(h2o)
h2o::h2o.init(nthreads = 1)

# EEG manipulation library in R (although very limited compared to signal processing libraries available in other languages, matlab might actually still be a leader in this specific area)
library(eegkit)

# some time series functions (that we only skim the depths of)
library(forecast)
library(tseries)

# just tidyverse libraries that should already be installed
library(dplyr)
library(reshape2)
library(purrr)
library(ggplot2)
```

## EEG Eye Detection Data

One of the most common types of medical sensor data (and one that we talked about during the lecture) are Electroencephalograms (EEGs).  
These measure mesoscale electrical signals (measured in microvolts) within the brain, which are indicative of a region of neuronal activity.
Typically, EEGs involve an array of sensors (aka channels) placed on the scalp with a high degree of covariance between sensors.

As EEG data can be very large and unwieldy, we are going to use a relatively small/simple dataset today from [this paper](http://ehrai.com/su/pdf/aihls2013.pdf).

This dataset is a 117 second continuous EEG measurement collected from a single person with a device called a "Emotiv EEG Neuroheadset".
In combination with the EEG data collection, a camera was used to record whether person being recorded had their eyes open or closed. 
This was eye status was then manually annotated onto the EEG data with `1` indicated the eyes being closed and `0` the eyes being open.
Measures microvoltages are listed in chronological order with the first measured value at the top of the dataframe.

Let's parse the data directly from `h2o`'s test data S3 bucket:
```{r dataset}
eeg_url <- "https://h2o-public-test-data.s3.amazonaws.com/smalldata/eeg/eeg_eyestate_splits.csv"
eeg_data <- dplyr::as_tibble(h2o::h2o.importFile(eeg_url))

# add timestamp
Fs <- 117 / dim(eeg_data)[1]
eeg_data <- eeg_data %>% dplyr::mutate(ds=seq(0, 116.99999, by=Fs), eyeDetection=as.factor(eyeDetection))
print(eeg_data %>% dplyr::group_by(eyeDetection) %>% dplyr::count())

# split dataset into train, validate, test
eeg_train <- eeg_data %>% dplyr::filter(split=='train') %>% dplyr::select(-split)
print(eeg_train %>% dplyr::group_by(eyeDetection) %>% dplyr::count())

eeg_validate <- h2o::as.h2o(eeg_data %>% dplyr::filter(split=='valid') %>% dplyr::select(-split))
eeg_test <- h2o::as.h2o(eeg_data %>% dplyr::filter(split=='test') %>% dplyr::select(-split))
```
**0** Knowing the `eeg_data` contains 117 seconds of data, inspect the `eeg_data` dataframe and work out approximately how many samples per second were taken?

**A:** The sample rate was approximately 128 Hz.

**1** How many EEG electrodes/sensors were used?

**A:** 14 sensors were collected from for this data, with those being the first 14 columns of the dataset.

### Exploratory Data Analysis

Now that we have the dataset and some basic parameters let's begin with the ever important/relevant exploratory data analysis.

First we should check there is no missing data!

```{r}
h2o::h2o.nacnt(h2o::as.h2o(eeg_data))
```

Great, now we can start generating some plots to look at this data within the time-domain.

```{r}
melt <- reshape2::melt(eeg_data %>% dplyr::select(-split), id.vars=c("eyeDetection", "ds"), variable.name = "Electrode", value.name = "microvolts")


ggplot2::ggplot(melt, ggplot2::aes(x=ds, y=microvolts, color=Electrode)) + 
  ggplot2::geom_line() + 
  ggplot2::ylim(3500,5000) + 
  ggplot2::geom_vline(ggplot2::aes(xintercept=ds), data=dplyr::filter(melt, eyeDetection==1), alpha=0.005)

```
**2** Do you see any obvious patterns between eyes being open (dark grey blocks in the plot) and the EEG intensities?

**A:** Whenever the eyes are initially opened, there's a spike upwards amongst many of the variables. This is especially apparent in F8 and AF4. F7 also seems to coincide with with this pattern, but with lower spikes and a tendency for the spikes  to be slightly earlier than the opening of the eyes, with a downwards spike or decline right after.

**3** Similarly, based on the distribution of eye open/close state over time to anticipate any temporal correlation between these states?

**A:** Yes, I would expect that 

Let's see if we can directly look at the distribution of EEG intensities and see how they related to eye status.

```{r}
melt_train <- reshape2::melt(eeg_train, id.vars=c("eyeDetection", "ds"), variable.name = "Electrode", value.name = "microvolts")

# filter huge outliers in voltage
filt_melt_train <- dplyr::filter(melt_train, microvolts %in% (3750:5000)) %>% dplyr::mutate(eyeDetection=as.factor(eyeDetection))

ggplot2::ggplot(filt_melt_train, ggplot2::aes(y=Electrode, x=microvolts, fill=eyeDetection)) + ggplot2::geom_boxplot()
```
Plots are great but sometimes so it is also useful to directly look at the summary statistics and how they related to eye status:


```{r}
filt_melt_train %>% dplyr::group_by(eyeDetection, Electrode) %>% 
    dplyr::summarise(mean = mean(microvolts), median=median(microvolts), sd=sd(microvolts)) %>% 
    dplyr::arrange(Electrode)
```

**4** Based on these analyses are any electrodes consistently more intense or varied when eyes are open?

**A:** It would seem that some average differences occur when looking at the variables with eyes opened and closed, but generally these differences are small and may not reach statistical significance. F7 is slightly higher in activation with eyes closed, but the median and mean differences are within each other's standard deviations.The O1 electrode sees a similar effect, with a median difference of 20 in favour of eyes closed closed. Interestingly, the medians are quite close together despite the medians being roughly a standard deviation apart. 

In the T8 electrode a similar effect is seen but in favour of eyes being open, with a median difference of 20, which is roughly a standard deviation apart as well. The means are quite close in this instance, however. 

Finally, the F8 electrode had a median and mean difference in favour of eyes open, with both being within the standard deviations. This is also the largest reported mean difference, being almost 20 uV. 

#### Time-Related Trends

As it looks like there may be a temporal pattern in the data we should investigate how it changes over time.  
First we will do a statistical test for stationarity:

```{r, warning=FALSE}
apply(eeg_train, 2, tseries::adf.test)
```
**5** Why are we interested in stationarity? What do the results of these tests tell us? (ignoring the lack of multiple comparison correction...)

**A:** We are interested in stationarity because, a general trend in the data over time, if it exists, would have to be accouted for when considering the potential effects of different components e.g. eye open/close state. It may be beneficial to remove or otherwise assess the general trend as its own component so that we can more closely look at which changes in activation are attributable or otherwise correlated with eye state. 
For these test results, the null hypothesis is that a "unit root" exists for the data, and the more negative the score the higher the rejection of this null. With a p-value of 0.01, these results indicate that the odds of observing an ADF value this negative with the null being true in actuality is at or less than 1%. Given this, the null is rejected at the 99% confidence level for all variables except for ds, which is the measure of the time at which the reading was taken. What this means is that there is support for the hypothesis of stationarity in all of the sensor variables.  

Then we may want to visually explore patterns of autocorrelation (previous values predict future ones) and cross-correlation (correlation across channels over time) using `forecast::ggAcf` function? 

```{r fig.width=11}
forecast::ggAcf(eeg_train %>% dplyr::select(-ds))
```

**7** Do any fields show signs of strong autocorrelation (diagonal plots)? Do any pairs of fields show signs of cross-correlation? Provide examples.

**A:** F7, FC5, T7, O1, O2, T8, FC6, and F4 all show varying degrees of autocorrelation. Unsurprisingly, autocorrelation is also found in the EyeDetection variable. Out of the sensor variables, of particular note are the correlations for F7, FC5, O1, and FC6. Which are some of the stronger correlations present in the chart overall. 

#### Frequency-Space 

We can also explore the data in frequency space by using a Fast Fourier Transform.  
After the FFT we can summarise the distributions of frequencies by their density across the power spectrum.
This will let us see if there any obvious patterns related to eye status in the overall frequency distributions.

```{r, fft_open}
eegkit::eegpsd(eeg_train %>% dplyr::filter(eyeDetection == 0) %>% dplyr::select(-eyeDetection, -ds), Fs = Fs, xlab="Eye Open")
```

```{r, fft_closed}
eegkit::eegpsd(eeg_train %>% dplyr::filter(eyeDetection == 1) %>% dplyr::select(-eyeDetection, -ds), Fs = Fs, xlab="Eye Closed")
```

**8** Do you see any differences between the power spectral densities for the two eye states? If so, describe them.

**A:** One of the most striking differences in spike pattern is the much more negative activation in channels 8 and 10 when the eyes are open. The power level repeatedly reaches around -40 dB in the second graph, while staying around 0 dB in the first. Similarly, channels 1, 9, and 13 have a moderately high and continuous positive activation of 40 dB when the eyes are open despite staying around 0 when the eyes are closed. Finally, the reverse is true of channels 6 and 14, with large positive activation when closed and moderate negative activation when open.

#### Independent Component Analysis

We may also wish to explore whether there are multiple sources of neuronal activity being picked up by the sensors.  
This can be achieved using a process known as independent component analysis (ICA) which decorrelates the channels and identifies the primary sources of signal within the decorrelated matrix.

```{r}
ica <- eegkit::eegica(eeg_train %>% dplyr::select(-eyeDetection, -ds), nc=3, method='fast', type='time')
mix <- dplyr::as_tibble(ica$M)
mix$eyeDetection <- eeg_train$eyeDetection
mix$ds <- eeg_train$ds
```
```{r}

mix_melt <- reshape2::melt(mix, id.vars=c("eyeDetection", "ds"), variable.name = "Independent Component", value.name = "M")
```
```{r fig.width=10}

ggplot2::ggplot(mix_melt, ggplot2::aes(x=ds, y=M, color=`Independent Component`)) + 
  ggplot2::geom_line() + 
  ggplot2::geom_vline(ggplot2::aes(xintercept=ds), data=dplyr::filter(mix_melt, eyeDetection==1), alpha=0.005) +
  ggplot2::scale_y_log10()
```

**9** Does this suggest eye activate forms an independent component of activity across the electrodes?

**A:** This plot suggests that the eye activity forms two components that are largely independent, in V1 and V2, which have very different looking spikes but which both follow a pattern of spikes with a clear relation to the opening and closing of the eyes. V2 very consistently shows activation in the form of a downward spike at the beginning of each shaded section (eyes opening). Although much more erratic, V1 also shows this same pattern, and has pronounced spikes that end slightly upward at the time of eyes opening. Upon closer inspection, these upwards spike endpoints are also present in V2, meaning that both components have the same relationship with the eye open/closed state. 

This would in my opinion mean that although we can identify components that clearly seem to relate to eye state, eye activity does not necessarily form one independent component, or at the very least that component is neither V1 nor V2, but instead a commonality between the two.

### Eye Opening Prediction

Now that we've explored the data let's use a simple model to see how well we can predict eye status from the EEGs:

```{r}
model <- h2o::h2o.gbm(x = colnames(dplyr::select(eeg_train, -eyeDetection, -ds)), 
                      y = colnames(dplyr::select(eeg_train, eyeDetection)),
                      training_frame = h2o::as.h2o(eeg_train),
                      validation_frame = eeg_validate,
                      distribution = "bernoulli",
                      ntrees = 300,
                      max_depth = 6,
                      learn_rate = 0.31)

print(model)

```

**10** What validation performance can you get with `h2o::h2o.xgboost` instead?

**A:** Hyperparameters were tested by slowly adjusting ntrees, max_depth, and learn_rate until performance stopped increasing. It was found that a learn rate of about 0.3 was ideal for both models, and performance/running time considerations determined the stopping point for the other two parameters, where performance continued to slightly increase for higher ntrees and 300 was determined to be an adequate stopping point. max_depth showed differing results depending on the value of ntrees but ultimately a depth of 6 was settled on. Strangely, gbm generally outperformed xgboost on AUC and AUCPR metrics, which were the main metrics that were focused on.

```{r}
model2 <- h2o::h2o.xgboost(x = colnames(dplyr::select(eeg_train, -eyeDetection, -ds)), 
                      y = colnames(dplyr::select(eeg_train, eyeDetection)),
                      training_frame = h2o::as.h2o(eeg_train),
                      validation_frame = eeg_validate,
                      distribution = "bernoulli",
                      ntrees = 300,
                      max_depth = 6,
                      learn_rate = 0.3)

print(model2)

```

**11** Using the best performing of the two models calculate the test performance

**A:** For some hyperparameter values, xgboost did outperform gbm, but these effects generally became less common for higher values of ntrees. The final comparison between the two models was made with the values of highest observed performance, which happened to be almost the same for the two models, but would probably differ if optimal values for ntrees and max_depth were to be reached. At the comparison point, gbm had slightly higher performance, and so its results are shown below:

```{r test}
perf <- h2o::h2o.performance(model = model, newdata = h2o::as.h2o(eeg_test))
print(perf)
```

**12** Describe 2 possible alternative modelling approaches we discussed in class but haven't explored in this notebook.

One alternative approach to modelling would be guassian process modelling, which, in comparison to an independent component analysis, involves the generation of many hypothetical components which could model the observed data, and then evaluating the many prospective components or functions on a number of points in order to generate a probable range of potential functions which could generate the observed data, giving something similar in effect to a confidence interval to the distributions it produces that is consistent with the observed data. 

Another possible approach would be a hidden markov model, which is a type of state-space model. A hidden markov model could be used here to explore trends within the data, such as associations between individual sensors and eye open/close state, and these trends can be observed using a system of timesteps. 
With this type of model, each of the timesteps would be considered a new state, and the predictions for a new state are made only based on the current state. Because there is a general regularity to the states, given a fairly consistent sample rate of 128 Hz, this model would find trends in the immediate case, rather than long term, which may or may not provide valuable insight into the data. The model probably wouldn't be ideal to use on its own, but through comparison to other methods some useful trend data may be found. For instance, it might be possible to predict when the eye state will change based on current sensor data using this method, if there are reliable predictors in the data just before a transition from eyes closed to open or vice versa. 